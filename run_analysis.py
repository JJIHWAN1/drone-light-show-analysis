#!/usr/bin/env python3
"""
ë“œë¡  ë¼ì´íŠ¸ ì‡¼ ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:
1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
2. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
3. íŠ¸ë Œë“œ ë¶„ì„
4. ì‹œê°í™” ìƒì„±
5. ë¦¬í¬íŠ¸ ì‘ì„±
"""

import sys
import os

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, 'src')
sys.path.insert(0, src_dir)

from data_processing import DroneSearchAnalyzer, save_analysis_results
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

def main():
    """ë©”ì¸ ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš ë“œë¡  ë¼ì´íŠ¸ ì‡¼ ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ ë° ë¶„ì„ê¸° ì´ˆê¸°í™”
    print("ğŸ“Š 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...")
    try:
        analyzer = DroneSearchAnalyzer("./data/naver_datalab_fixed.csv")
        print(f"   âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(analyzer.df)} ë ˆì½”ë“œ")
        print(f"   ğŸ“… ë¶„ì„ ê¸°ê°„: {analyzer.df['date'].min()} ~ {analyzer.df['date'].max()}")
        print(f"   ğŸ¢ ë¶„ì„ ì§€ì—­: {', '.join(analyzer.df['region'].unique())}")
    except Exception as e:
        print(f"   âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 2. ê¸°ë³¸ í†µê³„ ë¶„ì„
    print("\nğŸ“ˆ 2. ê¸°ë³¸ í†µê³„ ë¶„ì„...")
    basic_stats = analyzer.get_basic_statistics()
    
    print("   ì§€ì—­ë³„ ê¸°ë³¸ í†µê³„:")
    for region, stats in basic_stats.items():
        print(f"   ğŸ“ {region}:")
        print(f"      - í‰ê·  ê²€ìƒ‰ë¹„ìœ¨: {stats['mean']:.4f}")
        print(f"      - ìµœëŒ€ ê²€ìƒ‰ë¹„ìœ¨: {stats['max']:.4f}")
        print(f"      - í‘œì¤€í¸ì°¨: {stats['std']:.4f}")
        print(f"      - ë³€ë™ê³„ìˆ˜: {stats['cv']:.4f}")
    
    # 3. í”¼í¬ ë¶„ì„
    print("\nğŸ”¥ 3. ì£¼ìš” ê²€ìƒ‰ í”¼í¬ ë¶„ì„...")
    peaks = analyzer.detect_peaks(height_multiplier=2)
    
    for region, peak_info in peaks.items():
        print(f"   ğŸ“ {region}: {peak_info['peak_count']}ê°œ ì£¼ìš” í”¼í¬ ë°œê²¬")
        if len(peak_info['peak_dates']) > 0:
            max_peak_idx = np.argmax(peak_info['peak_values'])
            max_peak_date = peak_info['peak_dates'][max_peak_idx]
            max_peak_value = peak_info['peak_values'][max_peak_idx]
            print(f"      ğŸ† ìµœê³  í”¼í¬: {max_peak_date.strftime('%Y-%m-%d')} ({max_peak_value:.4f})")
    
    # 4. ìƒê´€ê´€ê³„ ë¶„ì„
    print("\nğŸ”— 4. ì§€ì—­ê°„ ìƒê´€ê´€ê³„ ë¶„ì„...")
    correlation_matrix = analyzer.calculate_correlation_matrix()
    
    print("   ì§€ì—­ê°„ ìƒê´€ê³„ìˆ˜:")
    regions = list(correlation_matrix.columns)
    for i in range(len(regions)):
        for j in range(i+1, len(regions)):
            corr_value = correlation_matrix.iloc[i, j]
            print(f"   ğŸ“Š {regions[i]} â†” {regions[j]}: {corr_value:.3f}")
    
    # 5. ì‹œê°„ì  íŒ¨í„´ ë¶„ì„
    print("\nğŸ“… 5. ì‹œê°„ì  íŒ¨í„´ ë¶„ì„...")
    patterns = analyzer.analyze_temporal_patterns()
    
    # ì›”ë³„ íŒ¨í„´
    monthly_peak = patterns['monthly'].groupby('region')['mean'].idxmax()
    print("   ì›”ë³„ íŒ¨í„´:")
    for region in analyzer.df['region'].unique():
        region_monthly = patterns['monthly'][patterns['monthly']['region'] == region]
        peak_month_idx = region_monthly['mean'].idxmax()
        peak_month = region_monthly.loc[peak_month_idx, 'month']
        peak_value = region_monthly.loc[peak_month_idx, 'mean']
        print(f"   ğŸ“ {region}: {int(peak_month)}ì›”ì´ ìµœê³  ({peak_value:.4f})")
    
    # 6. íŠ¸ë Œë“œ ë¶„ì„
    print("\nğŸ“ˆ 6. ì¥ê¸° íŠ¸ë Œë“œ ë¶„ì„...")
    trend_results = analyzer.perform_trend_analysis()
    
    for region, trend in trend_results.items():
        direction = "ìƒìŠ¹" if trend['slope'] > 0 else "í•˜ë½"
        significance = "ìœ ì˜í•¨" if trend['p_value'] < 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
        print(f"   ğŸ“ {region}:")
        print(f"      - íŠ¸ë Œë“œ: {direction} (ê¸°ìš¸ê¸°: {trend['slope']:.6f})")
        print(f"      - RÂ²: {trend['r_squared']:.4f}")
        print(f"      - í†µê³„ì  ìœ ì˜ì„±: {significance} (p={trend['p_value']:.4f})")
    
    # 7. ì´ìƒì¹˜ íƒì§€
    print("\nâš ï¸  7. ì´ìƒì¹˜ íƒì§€...")
    anomalies = analyzer.detect_anomalies(method='zscore', threshold=3)
    
    for region, anomaly_info in anomalies.items():
        print(f"   ğŸ“ {region}: {anomaly_info['count']}ê°œ ì´ìƒì¹˜ ë°œê²¬")
        if anomaly_info['count'] > 0:
            max_anomaly_idx = np.argmax(anomaly_info['values'])
            max_anomaly_date = anomaly_info['dates'][max_anomaly_idx]
            max_anomaly_value = anomaly_info['values'][max_anomaly_idx]
            print(f"      ğŸš¨ ìµœëŒ€ ì´ìƒì¹˜: {max_anomaly_date.strftime('%Y-%m-%d')} ({max_anomaly_value:.4f})")
    
    # 8. ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ 8. ë¶„ì„ ê²°ê³¼ ì €ì¥...")
    try:
        save_analysis_results(analyzer, "./results")
        print("   âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
        print("   ğŸ“ ì €ì¥ ìœ„ì¹˜:")
        print("      - ì „ì²˜ë¦¬ëœ ë°ì´í„°: ./results/data/")
        print("      - ë¶„ì„ ë¦¬í¬íŠ¸: ./results/reports/")
        print("      - ì‹œê°í™” ê²°ê³¼: ./results/figures/")
    except Exception as e:
        print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # 9. ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìš”ì•½
    print("\nğŸ¯ 9. ì£¼ìš” ë¶„ì„ ì¸ì‚¬ì´íŠ¸:")
    print("=" * 60)
    
    # ê°€ì¥ í™œë°œí•œ ì§€ì—­
    total_searches = analyzer.df.groupby('region')['ratio'].sum()
    most_active_region = total_searches.idxmax()
    print(f"ğŸ† ê°€ì¥ í™œë°œí•œ ê²€ìƒ‰ ì§€ì—­: {most_active_region}")
    
    # ì „ì²´ ìµœê³  í”¼í¬
    max_search_idx = analyzer.df['ratio'].idxmax()
    max_search_region = analyzer.df.loc[max_search_idx, 'region']
    max_search_date = analyzer.df.loc[max_search_idx, 'date']
    max_search_value = analyzer.df.loc[max_search_idx, 'ratio']
    print(f"ğŸ“ˆ ì „ì²´ ìµœê³  ê²€ìƒ‰ ê¸°ë¡: {max_search_region} ({max_search_date.strftime('%Y-%m-%d')}, {max_search_value:.4f})")
    
    # ê°€ì¥ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ì§€ì—­ ìŒ
    corr_values = []
    region_pairs = []
    regions = list(correlation_matrix.columns)
    for i in range(len(regions)):
        for j in range(i+1, len(regions)):
            corr_values.append(correlation_matrix.iloc[i, j])
            region_pairs.append((regions[i], regions[j]))
    
    max_corr_idx = np.argmax(corr_values)
    max_corr_pair = region_pairs[max_corr_idx]
    max_corr_value = corr_values[max_corr_idx]
    print(f"ğŸ”— ê°€ì¥ ìœ ì‚¬í•œ ê²€ìƒ‰ íŒ¨í„´: {max_corr_pair[0]} â†” {max_corr_pair[1]} (ìƒê´€ê³„ìˆ˜: {max_corr_value:.3f})")
    
    # ê³„ì ˆì„± íŒ¨í„´
    seasonal_avg = analyzer.df.groupby(['season', 'region'])['ratio'].mean().reset_index()
    peak_season = seasonal_avg.groupby('season')['ratio'].mean().idxmax()
    print(f"ğŸŒ¸ ê²€ìƒ‰ì´ ê°€ì¥ í™œë°œí•œ ê³„ì ˆ: {peak_season}")
    
    print("\nâœ¨ ë¶„ì„ ì™„ë£Œ! ëŒ€ì‹œë³´ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:")
    print("   cd src && python dashboard.py")
    print("   ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8050 ì ‘ì†")

if __name__ == "__main__":
    main()
